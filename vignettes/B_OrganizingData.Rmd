---
title: "Organizing Data for AnVIL Workflow"
author:
- name: Kayla Interdonato
  affiliation: Roswell Park Comprehensive Cancer Center, Buffalo, NY
- name: Martin Morgan
  affiliation: Roswell Park Comprehensive Cancer Center, Buffalo, NY
date: "`r Sys.Date()`"
output:
    BiocStyle::html_document:
        toc: true
        toc_float: true
vignette: >
  %\VignetteIndexEntry{B_OrganizingData}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

# Introduction

This vignette will walk the user through how to download the example dataset
that will be used for the analysis. Once it's downloaded we will show where the
data needs to be stored in order for the workflow to be able to locate it.

# Installation

The previous vignette [An Overview of AnVILBulkRNASeq](A_Overview.html)
demonstrates how to install the package. Refer to that vignette for installation
steps. The following command will load the package.

```{r setup}
library(AnVILBulkRNASeq)
```

# Using AnVIL

There is a package in Bioconductor that will provide all the necessary
functionality for communicating to the Google Bucket storage as well as the
AnVIL workspace. This package is named `r Biocpkg("AnVIL")`. The first step will
be to install the required packages so we can use their functions. To be sure we
are using the latest version of the `AnVIL` package, we will download it from
the Bioconductor GitHub.

```{r AnVIL_install}
pkgs = c("Bioconductor/AnVIL", "tidyverse", "dplyr") 
BiocManager::install(pkgs)
```

Then we will load the packages:

```{r load_AnVIL}
suppressPackageStartupMessages({
    library(AnVIL)
    library(tidyverse)
    library(dplyr)
})
```

# Download the data

We will be using Arabidopsis thaliana data from Ensembl to demonstrate this
workflow. The raw FASTA files are downloaded from Ensembl and saved to a
compressed .gz file. This compressed file will be needed for our workflow step
so we will copy the file to the available Google bucket. This is where we will
utilize `AnVIL`'s `gstuil_cp` function.

```{r download_fasta}
download.file("ftp://ftp.ensemblgenomes.org/pub/plants/release-28/fasta/arabidopsis_thaliana/cdna/Arabidopsis_thaliana.TAIR10.28.cdna.all.fa.gz",
"athal.fa.gz")
AnVIL::gsutil_cp("athal.fa.gz", AnVIL::avbucket())
unlink("athal.fa.gz")
```

Along with the compressed .gz file, the worflow step will also need the sample
specific paired FASTA files. These must be downloaded and made available on the
Google bucket.

```{r download_pairs}
base_url_1 =
"ftp://ftp.sra.ebi.ac.uk/vol1/fastq/DRR016/DRR0161%s/DRR0161%s_1.fastq.gz"
base_url_2 =
"ftp://ftp.sra.ebi.ac.uk/vol1/fastq/DRR016/DRR0161%s/DRR0161%s_2.fastq.gz"

urls1 = sprintf(base_url_1, 25:40, 25:40)
urls2 = sprintf(base_url_2, 25:40, 25:40)

lapply(seq_along(urls1), function(x) {
    download.file(urls1[x], basename(urls1[x]))
    download.file(urls2[x], basename(urls2[x]))
})

lapply(25:40, function(x) {
    AnVIL::gsutil_cp(paste0("DRR0161",x,"_*.fastq.gz"),
        file.path(AnVIL::avbucket(), "data", paste0("DRR0161",x))
    )
})
```

# Metadata file

We provide the corresponding metadata.csv for this sample dataset, but it should
be used as an example. When working through these vignettes, the Google bucket
addresses will be different for each user. The user will need to copy the Google
bucket address for the `fastq_1` and `fastq_2` files and add them to the
metadata file for their own samples. We will read in our metadata.csv that
already has the proper Google bucket locations added to it.

```{r metadata}
fl <- system.file("extdata", "metadata.csv", package = "AnVILBulkRNASeq")
tbl <- read.csv(fl)
```

## Pushing a local metadata file

Perhaps you have your own local metadata.csv file that you would like to
push/upload up to the Google bucket for the workflow. You can do this but will
need to do some initial set up. The gcloud SDK will need to be installed and the
`r Biocpkg("AnVIL")` vignette, section 2.3 Local use will explain how to do
this. Once the gcloud SDK is installed, the environment variable is defined, and
the installation is tested you can use the following code to push the
metadata.csv file to the cloud.

```{r local}
## set the workspace name
avworkspace_name("Bioconductor-Workflow-DESeq2")

## save the location of the available bucket
src <- avbucket()

## read in the local copy
## and reformat (not sure if this step is necessary)
dat <- read.csv("/Users/ka36530_ca/AnVILBulkRNASeq/inst/extdata/metadata.csv")
tbl <- tibble(dat)
tbl <- tbl %>% mutate(condition = as.factor(condition), type = as.factor(type))

## then import the tibble into the bucket
tbl %>% avtable_import()
```

If you go to the Data tab under the corresponding AnVIL workspace than you
should see the updated table, in our case it is the participant table.

# Next steps

Now that the data is downloaded and in it's proper location the next vignette
will walk through how to run the workflow that will quantify the raw fasta files
using Salmon.


# Session Information

```{r, eval = TRUE}
sessionInfo()
```
